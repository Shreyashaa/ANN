1

import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
def Sigmoid(t):
    return 1/(1+np.exp(-t))
t = np.linspace(-5, 5)
plt.plot(t, Sigmoid(t))
plt.title('Sigmoid Activation Function')
plt.show()

plt.figure(figsize=(8,4))
def RectifiedLinearUnit(t):
    lst=[]
    for i in t:
        if i>=0:
            lst.append(i)
        else:
            lst.append(0)
    return lst
arr = np.linspace(-5, 5)
plt.plot(arr, RectifiedLinearUnit(arr))
plt.title('Rectified Linear Unit Activation Function')
plt.show()

plt.figure(figsize=(8,4)) 
def Linear(x):
  return x
x=np.linspace(-5,5)
plt.plot(x,Linear(x))
plt.title("Linear Activation Funcation")
plt.show()

2.

import numpy as np 

def sigmoid (x):
    return 1/(1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

#Input datasets
inputs = np.array([[0,0],[0,1],[1,0],[1,1]])
expected_output = np.array([[0],[1],[1],[0]])

epochs = 10000
lr = 0.1
inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1

hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))   #weights and bias initialization
hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))
output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))
output_bias = np.random.uniform(size=(1,outputLayerNeurons)) 

print("Initial hidden weights: ",end='')
print(*hidden_weights)
print("Initial hidden biases: ",end='')
print(*hidden_bias)
print("Initial output weights: ",end='')
print(*output_weights)
print("Initial output biases: ",end='')
print(*output_bias)

for _ in range(epochs):
	#Forward Propagation
	hidden_layer_activation = np.dot(inputs,hidden_weights)
	hidden_layer_activation += hidden_bias
	hidden_layer_output = sigmoid(hidden_layer_activation)

	output_layer_activation = np.dot(hidden_layer_output,output_weights)
	output_layer_activation += output_bias
	predicted_output = sigmoid(output_layer_activation) 
 
 #Backpropagation
	error = expected_output - predicted_output
	d_predicted_output = error * sigmoid_derivative(predicted_output)
	
	error_hidden_layer = d_predicted_output.dot(output_weights.T)
	d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)  
 
 #Updating Weights and Biases
	output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr
	output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr
	hidden_weights += inputs.T.dot(d_hidden_layer) * lr
	hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr 

print("Final hidden weights: ",end='')
print(*hidden_weights)
print("Final hidden bias: ",end='')
print(*hidden_bias)
print("Final output weights: ",end='')
print(*output_weights)
print("Final output bias: ",end='')
print(*output_bias)

print("\nOutput from neural network after 10,000 epochs: ",end='')
print(*predicted_output)  

3.

import numpy as np

class ART1:
    def __init__(self, num_features, vigilance):
        self.num_features = num_features
        self.vigilance = vigilance
        self.weights = np.zeros((1, num_features))
        self.category = -1

    def train(self, x):
        while True:
            net = np.dot(self.weights, x)
            rho = np.sum(net) / self.num_features
            if rho >= self.vigilance:
                self.weights = np.vstack((self.weights, x))
                self.category += 1
                break
            else:
                x = np.multiply(x, net)
                if np.sum(x) == 0:
                    break

    def predict(self, x):
        net = np.dot(self.weights, x)
        rho = np.sum(net) / self.num_features
        if rho >= self.vigilance:
            return self.category
        else:
            return -1                   

               

art = ART1(4, 0.5)

art.train(np.array([1, 1, 0, 0]))
art.train(np.array([0, 0, 1, 1]))

category = art.predict(np.array([0, 1, 0, 0]))
print(category)

4.

import numpy as np

# for plotting the graphs
import matplotlib.pyplot as plt

# for implementing perceptron model
from sklearn.linear_model import Perceptron

x1 = [1, 0, 0, 1]
x2 = [1, 0, 1, 0]

x = [[1,1], [0,0], [0, 1], [1, 0]]
y = [1, 0, 0, 0]

plt.figure(figsize=(3, 3), dpi=80)
plt.xlabel("x1")
plt.ylabel("x2")
plt.scatter(x1, x2, c = y)

clf = Perceptron(max_iter=100).fit(x, y)

print("Classes of the model : ",clf.classes_)
print("Intercept of the decision boundary : ",clf.intercept_)
print("Coefficients of the decision boundary : ",clf.coef_)

ymin, ymax = -1,2
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(ymin, ymax)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plotting the decision boundary
plt.figure(figsize=(4, 4))
ax = plt.axes()
ax.scatter(x1, x2, c = y)
plt.plot(xx, yy, 'k-')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
plt.show()
